{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3b67d3-ad60-4dfd-8487-2232de4661a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import brevitas\n",
    "from torch.nn import BatchNorm2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Module\n",
    "from brevitas.nn import QuantLinear\n",
    "from brevitas.nn import QuantReLU\n",
    "from brevitas.nn import QuantIdentity\n",
    "from brevitas.nn import QuantConv2d\n",
    "from brevitas.core.quant import QuantType\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d550cf7-a50a-4f6a-a7de-8e3e63e91c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd5b0a8-9a52-4bf5-b13c-a560a48f8a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                           train = True,\n",
    "                                           transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                           download = True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                          train = False,\n",
    "                                          transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                                          download=True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53591a5a-6b26-4cb0-91f2-a7773c56561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lenet_Quant(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Lenet_Quant, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "        QuantConv2d(1, 6, kernel_size=(5,5), bias=False), #stride =1 padding = 0 by default\n",
    "        BatchNorm2d(6),\n",
    "        QuantReLU(),\n",
    "        MaxPool2d(kernel_size =2, stride = 2))\n",
    "        self.layer2 = nn.Sequential(QuantConv2d(6, 16, kernel_size = (5,5), bias = False),\n",
    "        BatchNorm2d(16),\n",
    "        QuantReLU(),\n",
    "        MaxPool2d(kernel_size=2, stride =2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "        QuantLinear(400, 120, bias = False),\n",
    "        QuantReLU(),\n",
    "        QuantLinear(120, 84, bias = False),\n",
    "        QuantReLU(),\n",
    "        QuantLinear(84, num_classes, bias = False))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.layer3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6968e41-605d-460c-bb9a-75464b805b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lenet_Quant(\n",
       "  (layer1): Sequential(\n",
       "    (0): QuantConv2d(\n",
       "      1, 6, kernel_size=(5, 5), stride=(1, 1), bias=False\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (output_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (weight_quant): WeightQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClampSte()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "          )\n",
       "          (scaling_impl): StatsFromParameterScaling(\n",
       "            (parameter_list_stats): _ParameterListStats(\n",
       "              (first_tracked_param): _ViewParameterWrapper(\n",
       "                (view_shape_impl): OverTensorView()\n",
       "              )\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsMax()\n",
       "              )\n",
       "            )\n",
       "            (stats_scaling_impl): _StatsScaling(\n",
       "              (affine_rescaling): Identity()\n",
       "              (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (restrict_scaling_pre): Identity()\n",
       "            )\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bias_quant): BiasQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): QuantReLU(\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (act_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "          (activation_impl): ReLU()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClamp()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "              (stats_input_view_shape_impl): OverTensorView()\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsPercentile()\n",
       "              )\n",
       "              (restrict_scaling): _RestrictValue(\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (clamp_scaling): _ClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "              )\n",
       "              (restrict_inplace_preprocess): Identity()\n",
       "              (restrict_preprocess): Identity()\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): QuantConv2d(\n",
       "      6, 16, kernel_size=(5, 5), stride=(1, 1), bias=False\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (output_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (weight_quant): WeightQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClampSte()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "          )\n",
       "          (scaling_impl): StatsFromParameterScaling(\n",
       "            (parameter_list_stats): _ParameterListStats(\n",
       "              (first_tracked_param): _ViewParameterWrapper(\n",
       "                (view_shape_impl): OverTensorView()\n",
       "              )\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsMax()\n",
       "              )\n",
       "            )\n",
       "            (stats_scaling_impl): _StatsScaling(\n",
       "              (affine_rescaling): Identity()\n",
       "              (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (restrict_scaling_pre): Identity()\n",
       "            )\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bias_quant): BiasQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): QuantReLU(\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (act_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "          (activation_impl): ReLU()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClamp()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "              (stats_input_view_shape_impl): OverTensorView()\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsPercentile()\n",
       "              )\n",
       "              (restrict_scaling): _RestrictValue(\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (clamp_scaling): _ClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "              )\n",
       "              (restrict_inplace_preprocess): Identity()\n",
       "              (restrict_preprocess): Identity()\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): QuantLinear(\n",
       "      in_features=400, out_features=120, bias=False\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (output_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (weight_quant): WeightQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClampSte()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "          )\n",
       "          (scaling_impl): StatsFromParameterScaling(\n",
       "            (parameter_list_stats): _ParameterListStats(\n",
       "              (first_tracked_param): _ViewParameterWrapper(\n",
       "                (view_shape_impl): OverTensorView()\n",
       "              )\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsMax()\n",
       "              )\n",
       "            )\n",
       "            (stats_scaling_impl): _StatsScaling(\n",
       "              (affine_rescaling): Identity()\n",
       "              (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (restrict_scaling_pre): Identity()\n",
       "            )\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bias_quant): BiasQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "    (1): QuantReLU(\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (act_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "          (activation_impl): ReLU()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClamp()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "              (stats_input_view_shape_impl): OverTensorView()\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsPercentile()\n",
       "              )\n",
       "              (restrict_scaling): _RestrictValue(\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (clamp_scaling): _ClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "              )\n",
       "              (restrict_inplace_preprocess): Identity()\n",
       "              (restrict_preprocess): Identity()\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): QuantLinear(\n",
       "      in_features=120, out_features=84, bias=False\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (output_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (weight_quant): WeightQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClampSte()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "          )\n",
       "          (scaling_impl): StatsFromParameterScaling(\n",
       "            (parameter_list_stats): _ParameterListStats(\n",
       "              (first_tracked_param): _ViewParameterWrapper(\n",
       "                (view_shape_impl): OverTensorView()\n",
       "              )\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsMax()\n",
       "              )\n",
       "            )\n",
       "            (stats_scaling_impl): _StatsScaling(\n",
       "              (affine_rescaling): Identity()\n",
       "              (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (restrict_scaling_pre): Identity()\n",
       "            )\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bias_quant): BiasQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "    (3): QuantReLU(\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (act_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "          (activation_impl): ReLU()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClamp()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "            )\n",
       "            (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "              (stats_input_view_shape_impl): OverTensorView()\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsPercentile()\n",
       "              )\n",
       "              (restrict_scaling): _RestrictValue(\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (clamp_scaling): _ClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "              )\n",
       "              (restrict_inplace_preprocess): Identity()\n",
       "              (restrict_preprocess): Identity()\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): QuantLinear(\n",
       "      in_features=84, out_features=10, bias=False\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (output_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (weight_quant): WeightQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClampSte()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "          )\n",
       "          (scaling_impl): StatsFromParameterScaling(\n",
       "            (parameter_list_stats): _ParameterListStats(\n",
       "              (first_tracked_param): _ViewParameterWrapper(\n",
       "                (view_shape_impl): OverTensorView()\n",
       "              )\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsMax()\n",
       "              )\n",
       "            )\n",
       "            (stats_scaling_impl): _StatsScaling(\n",
       "              (affine_rescaling): Identity()\n",
       "              (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (restrict_scaling_pre): Identity()\n",
       "            )\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bias_quant): BiasQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Lenet_Quant(num_classes)\n",
    "model.load_state_dict(torch.load('qlenet5.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac92ab29-ac9a-4dbb-beb6-d9a46fe8b4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:1255: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1758.)\n",
      "  return super(Tensor, self).rename(names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 99.12 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58937b4a-ee0b-4f82-bff8-2c18374cb5f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'brevitas.onnx' has no attribute 'export_finn_onnx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m export_onnx_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLenet5_Quant.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mbo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_finn_onnx\u001b[49m(model, input_shape, export_onnx_path)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'brevitas.onnx' has no attribute 'export_finn_onnx'"
     ]
    }
   ],
   "source": [
    "import brevitas.onnx as bo\n",
    "\n",
    "export_onnx_path = \"Lenet5_Quant.onnx\"\n",
    "input_shape = torch.rand(1,1,32,32)\n",
    "bo.export_qonnx(model, input_shape, export_onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "208bbedd-cac8-4702-a8ef-317ba7a6eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.visualization import showSrc, showInNetron\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "o_model = ModelWrapper(\"onnx_models/Lenet5_Quant.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad31c2df-e42e-4ca3-a7f5-d1bed29b12d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'onnx_models/Lenet5_Quant.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb3fd55db10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"onnx_models/Lenet5_Quant.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a06e688d-f975-4e2f-8d17-87eee985313b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'onnx_models/lenet5_quant_tidy.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb4ac0a8f70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "\n",
    "o_model = o_model.transform(InferShapes())\n",
    "o_model = o_model.transform(FoldConstants())\n",
    "o_model = o_model.transform(GiveUniqueNodeNames())\n",
    "o_model = o_model.transform(GiveReadableTensorNames())\n",
    "o_model = o_model.transform(InferDataTypes())\n",
    "o_model = o_model.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "o_model.save(\"onnx_models/lenet5_quant_tidy.onnx\")\n",
    "showInNetron(\"onnx_models/lenet5_quant_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d3318cb-a7e2-4660-bca0-a29a00d2fcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.14.1+cu116)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.1-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.2.0)\n",
      "Collecting torch==2.2.1\n",
      "  Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m440.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (4.9.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (1.12)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.13.1)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 KB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.2.0\n",
      "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (3.1.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchvision) (2.8)\n",
      "Collecting nvidia-nccl-cu12==2.19.3\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m412.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 KB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchvision) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchvision) (1.3.0)\n",
      "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2024.2.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 torch-2.2.1 torchvision-0.17.1 triton-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5c472-c82f-494c-b9f7-14516cbe4d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.pytorch import ToTensor\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "\n",
    "pmodel = ModelWrapper(\"onnx_models/lenet5_quant_tidy.onnx\")\n",
    "global_inp_name = pmodel.graph.input[0].name\n",
    "ishape = pmodel.get_tensor_shape(global_inp_name)\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = \"onnx_models/lenet5_quant_tidy_preproc.onnx\"\n",
    "bo.export_qonnx(totensor_pyt, ishape, chkpt_preproc_name)\n",
    "\n",
    "# join preprocessing and core model\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "pmodel = pmodel.transform(MergeONNXModels(pre_model))\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = pmodel.graph.input[0].name\n",
    "pmodel.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\n",
    "\n",
    "\n",
    "\n",
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "\n",
    "# postprocessing: insert Top-1 node at the end\n",
    "pmodel = pmodel.transform(InsertTopK(k=1))\n",
    "\n",
    "# tidy-up again\n",
    "pmodel = pmodel.transform(InferShapes())\n",
    "pmodel = pmodel.transform(FoldConstants())\n",
    "pmodel = pmodel.transform(GiveUniqueNodeNames())\n",
    "pmodel = pmodel.transform(GiveReadableTensorNames())\n",
    "pmodel = pmodel.transform(InferDataTypes())\n",
    "pmodel = pmodel.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "\n",
    "\n",
    "pmodel.save(\"onnx_models/lenet5_quant_tidy_pre_post_proc.onnx\")\n",
    "showInNetron(\"onnx_models/lenet5_quant_tidy_pre_post_proc.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5b735a9-9833-40e8-b9d0-4a14b561e8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Cleaning up model\n",
      "Step 2: Applying transformations\n"
     ]
    }
   ],
   "source": [
    "#new model\n",
    "from qonnx.util.cleanup import cleanup_model\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "new_model = ModelWrapper(\"onnx_models/lenet5_quant_tidy_pre_post_proc.onnx\")\n",
    "#necessary transformations\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.general import (\n",
    "    ApplyConfig,\n",
    "    GiveReadableTensorNames,\n",
    "    GiveUniqueNodeNames,\n",
    "    RemoveStaticGraphInputs,\n",
    "    RemoveUnusedTensors,\n",
    ")\n",
    "\n",
    "print(\"Step 1: Cleaning up model\")\n",
    "new_model = cleanup_model(new_model)\n",
    "print(\"Step 2: Applying transformations\")\n",
    "new_model = new_model.transform(ConvertQONNXtoFINN())\n",
    "new_model = new_model.transform(InferShapes())\n",
    "new_model = new_model.transform(FoldConstants())\n",
    "new_model = new_model.transform(GiveUniqueNodeNames())\n",
    "new_model = new_model.transform(GiveReadableTensorNames())\n",
    "new_model = new_model.transform(InferDataTypes())\n",
    "new_model = new_model.transform(RemoveStaticGraphInputs())\n",
    "new_model.save(\"onnx_models/lenet_clean.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9800772c-3e49-489c-9d5f-d4ccabcc5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "new_model=new_model.transform(ConvertQONNXtoFINN())\n",
    "new_model.save(\"onnx_models/lenet_finn_qonnx.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7900834-8722-40ba-8018-11141361d432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needs lowering\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.streamline import Streamline\n",
    "from qonnx.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from qonnx.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveScalarLinearPastInvariants\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.general import RemoveUnusedTensors\n",
    "\n",
    "new_model = ModelWrapper(\"onnx_models/lenet_finn_qonnx.onnx\")\n",
    "new_model = new_model.transform(absorb.AbsorbSignBiasIntoMultiThreshold())\n",
    "new_model = new_model.transform(Streamline())\n",
    "need_lowering = len(new_model.get_nodes_by_op_type(\"Conv\")) > 0\n",
    "if need_lowering:\n",
    "    print(\"Needs lowering\")\n",
    "    new_model = new_model.transform(LowerConvsToMatMul())\n",
    "    new_model = new_model.transform(MakeMaxPoolNHWC())\n",
    "    new_model = new_model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "    new_model = new_model.transform(MakeMaxPoolNHWC())\n",
    "    new_model = new_model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "new_model = new_model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "new_model = new_model.transform(Streamline())\n",
    "# absorb final add-mul nodes into TopK\n",
    "new_model = new_model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "new_model = new_model.transform(InferDataLayouts())\n",
    "new_model = new_model.transform(RemoveUnusedTensors())\n",
    "\n",
    "new_model.save(\"onnx_models/lenet5_quant_streamlined.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "114536e4-919c-431c-9140-82e06748a24a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 98] Address already in use",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfinn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisualization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m showSrc, showInNetron\n\u001b[0;32m----> 2\u001b[0m \u001b[43mshowInNetron\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43monnx_models/lenet5_quant_streamlined.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/vboxuser/workspace/finn/src/finn/util/visualization.py:61\u001b[0m, in \u001b[0;36mshowInNetron\u001b[0;34m(model_filename, localhost_url, port)\u001b[0m\n\u001b[1;32m     59\u001b[0m     port \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8081\u001b[39m\n\u001b[1;32m     60\u001b[0m localhost_url \u001b[38;5;241m=\u001b[39m localhost_url \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOCALHOST_URL\u001b[39m\u001b[38;5;124m\"\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m \u001b[43mnetron\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrowse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m IFrame(src\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocalhost_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mport\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/netron/server.py:322\u001b[0m, in \u001b[0;36mstart\u001b[0;34m(file, address, browse, verbosity)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, browse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    311\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Start serving model file at address and open in web browser.\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m        A (host, port) address tuple.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrowse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbrowse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbosity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/netron/server.py:299\u001b[0m, in \u001b[0;36mserve\u001b[0;34m(file, data, address, browse, verbosity)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m     address \u001b[38;5;241m=\u001b[39m _make_port(address)\n\u001b[0;32m--> 299\u001b[0m thread \u001b[38;5;241m=\u001b[39m \u001b[43m_HTTPServerThread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m thread\u001b[38;5;241m.\u001b[39malive():\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/netron/server.py:130\u001b[0m, in \u001b[0;36m_HTTPServerThread.__init__\u001b[0;34m(self, content, address, verbosity)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maddress \u001b[38;5;241m=\u001b[39m address\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m address[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(address[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver \u001b[38;5;241m=\u001b[39m \u001b[43m_ThreadedHTTPServer\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_HTTPRequestHandler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver\u001b[38;5;241m.\u001b[39mtimeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.25\u001b[39m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver\u001b[38;5;241m.\u001b[39mblock_on_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socketserver.py:452\u001b[0m, in \u001b[0;36mTCPServer.__init__\u001b[0;34m(self, server_address, RequestHandlerClass, bind_and_activate)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bind_and_activate:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_activate()\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/server.py:137\u001b[0m, in \u001b[0;36mHTTPServer.server_bind\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mserver_bind\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Override server_bind to store the server name.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[43msocketserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTCPServer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     host, port \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_address[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_name \u001b[38;5;241m=\u001b[39m socket\u001b[38;5;241m.\u001b[39mgetfqdn(host)\n",
      "File \u001b[0;32m/usr/lib/python3.10/socketserver.py:466\u001b[0m, in \u001b[0;36mTCPServer.server_bind\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_reuse_address:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39msetsockopt(socket\u001b[38;5;241m.\u001b[39mSOL_SOCKET, socket\u001b[38;5;241m.\u001b[39mSO_REUSEADDR, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 466\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserver_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver_address \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mgetsockname()\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 98] Address already in use"
     ]
    }
   ],
   "source": [
    "from finn.util.visualization import showSrc, showInNetron\n",
    "showInNetron(\"onnx_models/lenet5_quant_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "393beb37-fb66-40aa-9617-5200e983a86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'onnx_models/lenet5_quant_dataflow_parent.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb3fb64b850>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "\n",
    "# choose the memory mode for the MVTU units, decoupled or const\n",
    "mem_mode = \"decoupled\"\n",
    "\n",
    "new_model = ModelWrapper(\"onnx_models/lenet5_quant_streamlined.onnx\")\n",
    "new_model = new_model.transform(absorb.AbsorbSignBiasIntoMultiThreshold())\n",
    "new_model = new_model.transform(Streamline())\n",
    "need_lowering = len(new_model.get_nodes_by_op_type(\"Conv\")) > 0\n",
    "if need_lowering:\n",
    "    print(\"Needs lowering\")\n",
    "    new_model = new_model.transform(LowerConvsToMatMul())\n",
    "    new_model = new_model.transform(MakeMaxPoolNHWC())\n",
    "    new_model = new_model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "    new_model = new_model.transform(MakeMaxPoolNHWC())\n",
    "    new_model = new_model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "new_model = new_model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "new_model = new_model.transform(Streamline())\n",
    "# absorb final add-mul nodes into TopK\n",
    "new_model = new_model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "new_model = new_model.transform(InferDataLayouts())\n",
    "new_model = new_model.transform(RemoveUnusedTensors())\n",
    "\n",
    "new_model.save(\"onnx_models/lenet5_quant_dataflow_parent.onnx\")\n",
    "showInNetron(\"onnx_models/lenet5_quant_dataflow_parent.onnx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a103c25d-725b-4c7e-bef3-418e5e5e9950",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'showInNetron' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mshowInNetron\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx_models/lenet5_quant_dataflow_parent.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'showInNetron' is not defined"
     ]
    }
   ],
   "source": [
    "showInNetron(\"onnx_models/lenet5_quant_dataflow_parent.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "337a3fb8-d268-4915-8fa4-effb583b4cdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m parent_model \u001b[38;5;241m=\u001b[39m new_model\u001b[38;5;241m.\u001b[39mtransform(CreateDataflowPartition())\n\u001b[0;32m----> 3\u001b[0m sdp_node \u001b[38;5;241m=\u001b[39m \u001b[43mparent_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_nodes_by_op_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mStreamingDataflowPartition\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m sdp_node \u001b[38;5;241m=\u001b[39m getCustomOp(sdp_node)\n\u001b[1;32m      5\u001b[0m dataflow_model_filename \u001b[38;5;241m=\u001b[39m sdp_node\u001b[38;5;241m.\u001b[39mget_nodeattr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "parent_model = new_model.transform(CreateDataflowPartition())\n",
    "\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "# save the dataflow partition with a different name for easier access\n",
    "dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "dataflow_model.save(\"onnx_models/lenet5_quant_dataflow.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723840b-6029-4d94-a648-e02647e5d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(\"onnx_models/lenet5_quant_dataflow.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3355078-bbc7-4486-b34b-7e9d5afaf1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(\"onnx_models/lenet5_quant_dataflow.onnx\")\n",
    "fc0 = model.graph.node[1]\n",
    "fcw = getCustomOp(fc0)\n",
    "print(\"CustomOp wrapper is of class \" + fcw.__class__.__name__)\n",
    "fcw.get_nodeattr_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dce847-fb19-4a16-9ae9-44be16e64639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from qonnx.custom_op.registry import getCustomOp\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "\n",
    "model = ModelWrapper(\"onnx_models/lenet5_quant_dataflow.onnx\")\n",
    "fc_layers = model.get_nodes_by_op_type(\"MatrixVectorActivation\")\n",
    "# each tuple is (PE, SIMD, in_fifo_depth) for a layer\n",
    "folding = [\n",
    "    (1, 1, [64], [64]),\n",
    "    (1, 6, [64], [64]),\n",
    "    (1, 4, [64], [64]),\n",
    "    (1, 4, [64], [64]),\n",
    "    (1, 4, [64], [64]),\n",
    "]\n",
    "for fcl, (pe, simd, ififodepth, ofifodepth) in zip(fc_layers, folding):\n",
    "    fcl_inst = getCustomOp(fcl)\n",
    "    fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "    fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    fcl_inst.set_nodeattr(\"inFIFODepths\", ififodepth)\n",
    "    fcl_inst.set_nodeattr(\"outFIFODepths\", ofifodepth)\n",
    "# use same SIMD values for the sliding window operators\n",
    "swg_layers = model.get_nodes_by_op_type(\"ConvolutionInputGenerator\")\n",
    "for i in range(len(swg_layers)):\n",
    "    swg_inst = getCustomOp(swg_layers[i])\n",
    "    simd = folding[i][1]\n",
    "    swg_inst.set_nodeattr(\"SIMD\", simd)\n",
    "\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.save(\"onnx_models/lenet5_folded.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
